{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import initialize_logging, load_config\n",
    "\n",
    "config = load_config(config_path=\"./config/config.yaml\")\n",
    "initialize_logging(config_path=\"./config/logging_config.yaml\", debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\imars\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2024-11-03 13:15:46,310\", \"level\":\"WARNING\", \"name\":\"xformers\", \"message\":\"WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.4.1+cu124)\n",
      "    Python  3.8.10 (you have 3.8.5)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\"}\n",
      "{\"timestamp\":\"2024-11-03 13:15:46,422\", \"level\":\"INFO\", \"name\":\"numexpr.utils\", \"message\":\"Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\"}\n",
      "{\"timestamp\":\"2024-11-03 13:15:46,423\", \"level\":\"INFO\", \"name\":\"numexpr.utils\", \"message\":\"NumExpr defaulting to 8 threads.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\imars\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xformers\\ops\\swiglu_op.py:107: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(cls, ctx, x, w1, b1, w2, b2, w3, b3):\n",
      "c:\\Users\\imars\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\xformers\\ops\\swiglu_op.py:128: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(cls, ctx, dx5):\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances_chunked\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "from adan import Adan\n",
    "\n",
    "from classification.data_model import BatchDict, Postfix, TestResults, ValDict\n",
    "from classification.early_stopper import EarlyStopper\n",
    "from classification.modules import ViTForImageClassificationCustom\n",
    "from classification.utils import (\n",
    "    calculate_ranking_metrics,\n",
    "    dataloader_factory,\n",
    "    dir_checker,\n",
    "    reduce_func,\n",
    "    save_best_log,\n",
    "    save_logs,\n",
    "    save_predictions,\n",
    "    save_test_predictions\n",
    ")\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import wandb\n",
    "t_loader = dataloader_factory(config=config, data_split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in t_loader:\n",
    "    print(batch['anchor'].shape)\n",
    "    print(batch['positive'].shape)\n",
    "    print(batch['negative'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
